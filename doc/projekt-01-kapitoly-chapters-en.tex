% This file should be replaced with your file with an thesis content.
%=========================================================================
% Authors: Michal Bidlo, Bohuslav Křena, Jaroslav Dytrych, Petr Veigend and Adam Herout 2019

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it and change
% \documentclass[../projekt.tex]{subfiles}
% \begin{document}

\chapter{Introduction}
%\todo{Gramar check}
%\todo{Style document}
This chapter summerizes the motivation and objectives of this thesis, the solution taken to achieve the objectives and the results of the thesis. In the last section the structure of the thesis is presented.

\section{Motivation and Objectives}
This work was instigaten by Honeywell International Inc. \cite{honeywell}. The company currently uses IBM Doors as a solution for for requirement management. The company is exploring the option and considering to switch to a new solution - Jira R4J. Honeywell currently provides its clients access to requirements stored in IBM Doors via the OSLC interface for Requirement Management Specification, which comes with IBM Doors. The company is interested in providing the same access to requirements stored in Jira R4J. The goal of this thesis is to explore and create a solution that will allow Honeywell to provide its clients access to requirements stored in Jira R4J via the OSLC interface for Requirement Management Specification.

\section{Solution}
There are two ways to add OSLC support to a web application, either as a addon or standalone web application. After careful consideration a decision was made to create the adaptor as a standalone web application. The main reason for this decision was to reduce coupling between the web application and the OSLC interface. This allows for easier maintenance and development, as well as the possibility to use the OSLC adaptor with other web applications, beside Jira R4J, in the future. 

During the initial design phase, it was also decided to split the adaptor into two separate adaptors, one for Jira, responsible for the main functionality specified by the Requirement Management Specification, and one for R4J, providing additional functionality, such as the ability to create folders and link requirements to folders. This decision was made in order explore the possibility of not needing to use the R4J pluggin at all, and instead use Jira directly. This also leads to the final solution being generic and less coupled to specific web application, allowing for easier enhancement or replacement of the underlying web application in the future.

Both adaptors were created using Eclipse Lyo, a project containing SDKs and other utilities, used for easier development of OSLC applications. Lyo Designer was used to create the Domain and Toolchain models, modeling the data and capabilities of both adaptors. These models were then fed to Code Generator to generate the code skeletos, compliant with the OSLC specification, for the adaptors. The generated skeletons were then filled with the actual code, implementing the functionality of the adaptors, and extended with additional functionality, such as OAuth2 authentication.

\section{Results}
Both adaptors were created as standalone REST based Java web applications, build on the Maven framework. During the developement, a collection of HTTP requests was created in Postman, detailing the example usage of created adaptors. The functionality of both adaptors was verified by end-to-end testing, utilising the Postman Test utility. For further verification of the base capability of Requirement Management Specification, implemented in Jira adaptor, a basic Python client was developed, providing the option to download and upload all of the requirements, in specified project, in the ReqIF format. 

The mapping of the data provided by Jira and R4J API to the OSLC format, was done as genericly as possible to accomande all posible use cases, which differ a lot between different companies. During the developement some issues and missing functionality were discovered in the R4J API, resulting in non-optimal implementation of some of the functionality of R4J adaptor.

The utilities and SDKs provided by Eclipse Lyo were very useful in the development and design stages of the adaptors, greatly reducting the time needed to adopt the OSLC standard. However, the documentation and examples provided by the Lyo project were outdated and not detailed enough, also lacking some deeper explanation of important concepts. The contents of the Requirement Management Specification were also not documented sufficently, compared to for example the Automation Domain Specification, resulting in difficulties in the comprehension of the specification.

\section{Structure of the Thesis}
\todo{Write after thesis structure is finalized}

%=========================================================================

\chapter{Background}
%%\todo{Gramar check}
%%\todo{Style document}
This chapter briefly covers the basics of requirements management and its fundamental concepts, as well as basic overview of Jira software and Requirements for Jira plugin. Information provided in this overview are important for understanding the requirements and needs of the adaptor. 

\section{Requirements Management}
The aim of requirement management is verify the acomplishment of the project's goals and objectives. The process is divided into several succesive stages, the most essectial being analysis, documentation, tracing and change control. The purpose of this process is to track the requiremnents and their status, identify inconsistencies and provide overview of the project progress to concerned stakeholders \cite{requirements_management}.

One of the most important parts of requirement management is traceability. It is the ability to track and assess the state of the requirement and its changes during the developement lifecycle, providing an audit trail, usualy used for reporting to stakeholders. Traceability can be achieved by linking requirements to other requirements, test cases or build stories.

\subsection*{Requirements}
Requirements are the foundation for determining the needs of system stakeholders and system itself. They represent a condition or capability
that must be met by a system or product \cite{IEEE_24765-2017}.

They can be divided into two main categories, functional and non-functional requirements. Functional requirements describe the system behavior or product features (eg. system will send email with forgotten password prompt when requested by user), while non-functional requirements usually specify the system's performace or product properties (eg. a taks has to be completed under 200 ms).

\section{Requirement Management Systems}
Requirement Management System is a software tool, used for management of the project requirements during the developement lifecycle. Currently, there are several different requirement management systems, developed by competing companies, available on the market. Some of the most popular ones are IBM Doors and Jira Software with Requirements for Jira plugin. A high level overview for these options is provided in the following sections, as they are important in the context of this thesis.

\subsection*{IBM Doors}
IBM Doors \cite{ibm_doors} is requirement management system developed by IBM \cite{ibm}. It is a complex tool, designed to handle large projects, with many requirements and stakeholders. The tool is commonly used in aerospace, defense and automotive industries. It provides a wide set of features to aid in requirement management process, such as requirement traceability, collaboration and ease of integration with other IBM tools. It is also designed in a way to help companies comply with industry standards and regulations for requirement management and project developement. All of the previously mentioned features are available through web user interface, REST API as well as native OSLC interface.

\subsection*{Jira Software and Requirements for Jira}
Jira Software \cite{jira} is issue tracking and project management tool developed by Atlassian \cite{atlassian}. Compared to IBM Doors, it provides more user friendly interface and is more flexible, as its functionality spans across many different areas, such as project management, test management and bug tracking. It also allows the end customers to extend the capabilities of their Jira instance by adding plugins, developed by Atlassian or third party developers, augmenting the original functionality or adding completely new features. The contents of Jira and its functionality are accessible through web user inteface or REST API \cite{jira_api} with BASIC or OAuth2 authentication, further described in chapter \ref{chapter:authentication}. It does not provide a native OSLC interface, but the support for OSLC can be added by installing a third party plugin (eg. OSLC Connector for Jira \cite{oslc_connector_for_jira} for Change Management Specification).

Requirements for Jira (R4J) \cite{requirements_for_jira} is a native Jira plugin, extending the capabilities of Jira by adding support for requirements management by making use of Jira issues. It provides the ability to manage requirement by createing a folder structure, enabling importing and exporting in the reqIF format, enabling traceability, by utilizing the Jira link functionality, and adding the option to export these links into comperehensive traceability matrix. All of these features are available both from the Jira web user interface as well as from the R4J REST API \cite{r4j_api}. Because the requirement management system provided by R4J is a part of Jira, it allows for better traceability, as the requirements can not only be linked to other requirements, but also to other Jira issue types, such as bugs, stories, test cases and many more.

\section{Requirements Interchange Format}
Requirements Interchange Format (ReqIF) \cite{reqif_standard} is a XML based format used for transfering and sharing requirements between requirement managment systems or other tools. The format defines standardisated way to describe requirements, their attributes and properties, releations between requirements and hierarchical structure, in which the requirements are contained. Each object, type, attribute and releations contains an unique identifier, by which it can be referenced from other objects.

The document is divided into two parts, THE-HEADER, containing metadata information about the requirement collection, and CORE-CONTENT, which is then further split into five sections, each containing different information about the requirements:

\begin{itemize}
  \item DATATYPES -- definitions of datatypes used in the document
  \item SPEC-TYPES -- definitions of types of requirements, its attributes and releations, the datatype of the attribute is defined by a reference to a definition in DATATYPES
  \item SPEC-OBJECTS -- collection of requirements and its properties, described in a format specified in SPEC-TYPES
  \item SPEC-RELATIONS -- collection of releations between requirements, described in a format specified in SPEC-TYPES
  \item SPECIFICATIONS -- requirements hierarchical tree structure
\end{itemize}

%=========================================================================

\chapter{Authentication}
\label{chapter:authentication}
%\todo{Gramar check}
%\todo{Style document}
Authentication is the process of verifying the identity of a person or user. It is used to restrict the server resources or functionality to only authorized users or specific groups of users. Several different authentication methods exists, each with its own advantages and disadvantages. This chapter gives a brief overview of BASIC and OAuth authentication methdos, which are used in this thesis.

\section{Basic Access Authentication}
BASIC (Basic Access Authentication) was first defined as a part of HTTP 1.0 specification \cite{http1.0_w3}, but the standard has been since superceeded and redefined as part of its own RFC \cite{basic_auth_rfc}. As its name suggest, BASIC is the most fundamental method of verifying the identity of client against server. It utilisis the HTTP Authorization header and provides server with the credentials of the client in form of Authorization: Basic <credentials>. The credentials are provided as a string, encoded in base64 \cite{base64_rfc}, containing the username and password joined by a collon.

Main advantage of using BASIC authentication is its ease of implementation. It is supported by most of available frameworks and does not require any cryptographic operations. However, it is also the most vurnerable one, as the credentials are sent in plain text, which makes it susceptible to interception by third party and potential creadentials theft. Most of this insecurities can be mitigated by using TLS to encrypt the communication between client and server, but it is still not a recomanded method for authentication in production environments.

\section{Open Authorization}
OAuth (Open Authorization) is a standard for delegated authentication and authorization, which stemmed from the need to enable the end users to be able to authenticate in the third party applications and services, using their creadentials from other service, which acts as the authorization server. Originaly, this was done by sharing the creadentials with the third party application, which then used them to authenticate the user. This method was vulnerable to the same security issues as BASIC authentication and on top of that it also allowed the third party application to access and read the users creadentials. OAuth was created to mitigate this problems and create a standardisated way for services to offer authentication and authorization to third party applications, without the need to share the users creadentials between them. The standard was first introduced as OAuth 1.0 \cite{oauth1_rfc}, which used asymmetric cryptography to encrypt and verify the creadentials, but was later superceeded by OAuth 2.0 \cite{oauth2_rfc}, which is easier to implement, as it leaves the encryption and verification of the creadentials origin to TLS protocol. The next section provides a brief overview of OAuth 2.0 and its authentication workflow.

\subsection{OAuth 2.0}
%what is oauth 2.0
OAuth 2.0 is a standard published as a reaction to new use cases. It is now backwards compatible with OAuth 1.0. The standard enforces that all of the comunication between the client and the authorization server is done using HTTPS. 

Several diferent roles are defined by the standard:
\begin{itemize}
  \item Resource owner -- entity capable of granting access to a protected resource
  \item Resource server -- server containing the resources, capable of authorization using the access tokens
  \item Client -- third party application, which is requesting access to resources on the resource server on behalf of the resource owner
  \item Authorization server -- issues access tokens to resource owner after successful authentication and authorization
\end{itemize}

\subsection*{Authorization Code Grant}
The standard also defines four authorization grants, ways for client to abtain the access token. For simplicity only the most common one, Authorization Code Grant, is described, as it is the one used in this thesis.

Authorization Code Grant is a two step process to obtain the access token. The user is first rediced to the authorization server, with a request to obtain authorization code. The authorization server authenticates the user and asks him for approval to grant the client specified scope of access to resources on the resource server. After the approval, the user is redirected back to client, with the authorization code. The client the uses this to make a request to the authorization server to obtain th access token. The authorization server then verifies the authorization code and issues the access token.

\todo{Add OAuth 2.0 Authorization Code Grant workflow diagram}

\subsection*{Proof Key for Code Exchange}
PKCE (Proof Key for Code Exchange) \cite{pkce_rfc} builds on top of Authorization Code Grant to further secure the process. It adds a secret called Code Verifier. Code Verifier is then to transformed into a value called Code Challage, which is send together with the authorization code request. Code Verifier is then send together with authorization code as a part of the access token request. The authorization server issues a new access token, only if the received Code Verifier matches the one used to generate the Code Challange. This removes the risk of the authorization code being intercepted and used to generate the access token, as the authorization server will not create the access token without the Code Verifier.

\todo{Add OAuth 2.0 PKCE workflow diagram}

\subsection*{Access and Refresh Tokens}
After successful authorization against the authorization server, the client is issued an access token. The client then uses this token to access the resources available on resource server, by sending it with each HTTP request in the Authorization header. Access tokens can be either an opaque string, called bearer token, or sender-constrained token. Sender-constrained tokens can be used only to authorize request send by same client, who the token was issued to. This is usually achieved by asymmetric cryptography of the token. To comply with the standard, the token cannot convey user identity or any other information about the user.

Together with the access token, the authorization server also issues a refresh token, which can be used to generate a new access token, without user interaction with the authorization server. This allows the authorization server to issue access tokens with shorter expiration time, reducting the impact of the token being intercepted and stolen.

\subsection*{Scopes}
Compared to OAuth 1.0, OAuth 2.0 also introduces the concept of OAuth Scopes, allowing the authorization server to issue access tokens with various limitations on access to resources. The scopes are defined by the resource server. The requested scope is part of the authorization request to authorization server and is bind to the access token generated as a result of the authorization.

\subsection*{OpenID Connect}
OIDC (OpenID Connect) \cite{oidc} is an extension of OAuth 2.0 allowing to authenticate the user against the authorization server as well as obtain basic user profile information, about the authenticated user.

%=========================================================================

\chapter{OSLC -- Open Services for Lifecycle Collaboration}
%\todo{Gramar check}
%\todo{Style document}
This chapter provides a brief overview of the OSLC (Open Services for Lifecycle Collaboration) \cite{oslc}, its fundamental technologies, and the Core and Requirement Management specifications, which are used in this thesis.

\section{Overview}
OSLC \cite{oslc} is an OASIS Open Project \cite{oasis_open} responsible for developing a set of open specifications, which are used for easier integration of software tools. More detailed overview of the OSLC project can be found in the OSLC Primer \cite{oslc_primer}.

The initiation of the OSLC project was driven by increasing number of software tools, which are used in software development lifecycle. These tools are usually developed by different organizations, which leads to the problem of difficult tool integration. In the past, this was solved by developing specific translators and adaptors for each tool, which was a time consuming and expensive process. OSLC was created to solve this problem, by creating a set of open specifications to integrate the resources managed by the software tools into the web of data.

OSLC offers two different methods of data integration - Linking data via HTTP and Linking Data via HTML User Interface \cite{oslc_primary_integration_techniques}.

\subsection*{Linking data via HTTP}
Linking of the data via HTTP is based on OSLC definied common tool protocol for accessing, creating, updating and deleting resources. The protocol is based on internet technologies and standards, such as REST, RDF and Linked Data, described in section \ref{sec:oslc_fundamental_technologies}. It allows any other tool, that implements the same specification, to access any of the managed resources. Linking of the data is done by referencing resources by HTTP URIs in the representations of other data.

\subsection*{Linking Data via HTML User Interface}
\label {sec:oslc_linking_data_via_html_user_interface}
OSLC protocol can be used to link data via HTML user interface as well, making use of the REST code on demand contstraint. This allows the client to access and display fragments of a existing user interface, provided by the tool, without the need to implement the user interface itself. This delegated user interface then enables the client to access the resources managed by the tool.

\section{Fundamental Technologies}
\label {sec:oslc_fundamental_technologies}
OSLC is build on top of several fundamental technologies. This section list these technologies and introduces briefly each of them.

\subsection*{REST}
REST (REpresentational State Transfer) \cite{rest} is a web software architecture style, describing set of contstraints and properties, which should be followed in comunication between computer systems, most commonly between client and server. In REST architecture the server is responsible for exposing a common interface, which allows the client to access resources by using standard HTTP methods. Each resource, the server is providing, has an unique identifier, that allows the resource to be identified unambiguously and completely. When requested, the server responds with a representation of the resource. The client can then use this resource to create new resources and update or delete existing resources. The comunication between the server and the client is stateless, meaning every request the server receives can be fully understood in isolation, without the context of previous requests. Most common HTTP methods used in REST are POST, GET, PUT and DELETE, which corresponds to the CRUD operations (Create, Read, Update and Delete).

\subsection*{RDF}
RDF (Resource Description Framework) \cite{rdf} is a W3C (World Wide Web Consortium) \cite{w3c} standard for representing data on web. It describes resources in the form of a directed graph, where information about each element are represented as a triplets. Tripletes are statements about the resource composed of subject, predicate and object. The subject describes the the resource, and the predicate specifies its properties and relationships, between the subject and the object. Most widely used RDF serialization formats are Turtle \cite{turtle}, RDF/XML and RDF/JSON.
\todo{This might use an image}

\subsection*{Linked Data}
Linked Data \cite{linked_data} are structured data containing references to other data. This enables computers to query and interpret the data, allowing the internet to become one big database. The main principles of Linked Data are \cite{linked_data_design_issues}:
\begin{enumerate}
  \item URIs \cite{uri_rfc} are used as names to identify things
  \item People can lookup things using HTTTP URIs
  \item Information returned as a result of the search are provided in an open standard format (for example RDF)
  \item Returned information contain more URIs, enabling discovery of other things
\end{enumerate}

\section{OSLC Specifications}
OSLC defines a set of open specifications for integrating software tools. OSLC consists of multiple working groups, which are each responsible for development of a specific specification. There are two types of specifications - Core and Domain. The Core specification provides a basis for the Domain specifications, which are then focused on a specific field, for example Requirement Management, Change Management, Configuration Management, etc. This section provides a summary of the OSLC Core and Requirement Management specifications, which are used in this thesis.

\subsection{OSLC Core Specification}
\todo{https://docs.oasis-open-projects.org/oslc-op/core/v3.0/os/images/oslc-architecture.png}
At the time of writing this thesis, the current version of OSLC Core Specification \cite{oslc_core_specification} is 3.0. The OSLC Core Specification defines set of common principles, capabilities and restrictions, which should be common accross all OSLC Domain Specifications. Specific OSLC Domain Specification will then describe, which of these capabilities are required or optional for conformance with the specification. It also introduces several resource types and properties with the namespace http://open-services.net/ns/core\# and prefix oslc. In the follwoing sections a basic overview of the core concepts is provided.

\subsection*{Resource Shape}
OSLC works with resources, which are uniquequly identified by an URI \cite{uri_rfc}, and are represented by RDF triples. Resource Shape \cite{oslc_core_resource_shape} is a resource of type oslc:ResourceShape, that describes the contents and contstraints of other resources.

\todo{Possible image: https://docs.oasis-open-projects.org/oslc-op/core/v3.0/os/images/shapes-overview.png - probably remake without the blue boxes}

Each Resource Shape has a defined set of Properties, of type oslc:Property, which specify the value type and cardinality of the property. The value type of the property can be either a reference to another Resource Shape, or a basic data type. Cardinality of the Property specifies if the Property is required, optional or can be present multiple times. OSLC Core Specification defines these basic data types: XMLLiteral, boolean, dateTime, decimal, double, float, integer, string and langString.

\subsection*{Discovery}
For the reasons of flexibility, and to reduce coupling, the OSLC Core Specification does not specify unequivocally which capabilities the server has to provide. Instead it offers a mechanism for incremental discovery of services and capabilities, the target server has implemented.
\todo{This could also use some image/diagram (https://docs.oasis-open-projects.org/oslc-op/core/v3.0/os/images/CoreDiscoveryUML.png)}
The server always has to specify the starting point of discovery, which is the Service Provider Catalog. The Service Provider Catalog is a resource containing a list of available Service Providers, which then contain all of the available Services. From there, the client is able to find the URIs for Creation Factories, Dialogs and Query services. For additional information about the Discovery mechanism can be found in the Discovery section of the OSLC Core Specification \cite{oslc_core_discovery}.

\subsection*{Basic Capabilities}
OSLC Core Specification defines basic CRUD (Create, Read, Update, Delete) operations for resources, however the decesion about which of these operations are required or optional for which resource is specified in each of the OSLC Domain Specifications. Read, Update and Delete operations are performed by their respective HTTP request method to the URI of the target resource. Create operation is performed by a POST operation to the Creation Factory URI for specific resource. 

\subsection*{Delegated UI}
OSLC Core Specification introduces Delegated UI \cite{oslc_core_delegated_ui} for resource creation -- Creation Dialog, and resource selection -- Selection Dialog. Both of these dialogs are examples of linking of the data via HTML mentioned in \ref{sec:oslc_linking_data_via_html_user_interface}. Dialogs are returned as a combination of HTML iframe and JavaScript code. The decesion about which of these dialogs are required or optional is again left up to the OSLC Domain Specification.

\subsection*{Query Capability}
As OSLC server manages large amount of resources, it has to provide a way for clients to search and filter these resourecs. OSLC Core Specification specifies a mechanism for this, called OSLC Query \cite{oslc_core_query}. OSLC Query allows clients to lookup a set of resourecs by performing a GET or POST requests on a oslc:queryBase URI. It offers two separate capabilities, a full-text search, identified by oslc.searchTerms parameter, and a query search for resources containing specifiec properties and values, identified by oslc.where parameter. Each query search must consist of at least one property, comparison operator and value. The result of the search is returned as a resource of type oslc:QueryResult, which contain a list of references to the resourecs found.
\todo{Maybe some XML example here for query}

\subsection*{Authentication and Error Responses}
OSLC Core Specification provides guidance on how to handle authentication and error resposes. Allowed authentication methods are Basic Authentication and OAuth. All error resonses should be returned as a resource of type oslc:Error \cite{oslc_core_error}, which contains a human-readable message and a machine-readable error code. This enables clients to handle errors in a generic way.

\subsection{OSLC Requirement Management Specification}
At the time of writing this thesis, the current version of OSLC Requirement Management Specification \cite{oslc_requirement_management_specification} is 2.1. The specification builds on top of the OSLC Core Specification and specifies, which of the capabilities are required or optional for conformance with the specification. The main goal is to provide an extensive, but not limiting, interface for requirement management system and support a wide range of integration scenarios. One of the main requirements for an OSLC RM Server is the ability to accept and return resources in RDF/XML, XML and JSON. It also introduces new resource types, in the namespace of http://open-services.net/ns/rm\# and with prefix oslc\_rm. These resource types are described in the following sections.

\subsection*{Requirement}
oslc\_rm:Requirement \cite{oslc_rm_requirement} is a resource shape used for describing the statement of need. \todo{mabye I want to refer this to the section about requirement management requirement} Requirement Management Specification defines extensive set of properties and contstraints \cite{oslc_rm_requirement_constraints} for the requirement shape, but there are few that are especialy important in the context of this work. 

Each individual requirement should have a title, usualy containing the name of the requirement, and a description, which consists of the actual statement of need. These two requisities realized by the properties dcterms:title and dcterms:description.

Requirements should also be able to reference other requirements or requirement collections, which are related to them. This is done by the properties oslc\_rm:decomposedBy and oslc\_rm:decomposes, the difference being the direction of the relationship.

\subsection*{Requirement Collection}
oslc\_rm:RequirementCollection \cite{oslc_rm_requirement_collection} is a resource shape used for describing a collection of requirements, which constitute some statement of need. Requirement Management Specification again defines the constraints and properties for this resource shape \cite{oslc_rm_requirement_collection_constraints}, but they are nearly the same as for oslc\_rm:Requirement resource shape.

\section{Eclipse Lyo}
Eclipse Lyo \cite{eclipse_lyo} is an open source project hosted by the Eclipse Foundation \cite{eclipse} and developed by the OSLC community. It provides a Java SDK, as well as other utilities, to enable easier adoption of the OSLC technologies and better developer experience. The following sections gives a concise summary of the key components of Eclipes Lyo.

\subsection*{OSLC4J SDK}
OSLC4J Software Developement Kit (available at Maven Repository \cite{maven_oslc4j}) is a set of Java libraries used for building OSLC compliant REST-based servers and clients. It includes support for common OSLC capabilities, resource shapes and service provider documents and marshaling and unmarshaling of resources to Java objects. 

\subsection*{Lyo Designer}
Lyo Designer \cite{lyo_designer} is Eclipse plugin used for graphical design of OSLC adaptors. It offers the capability to model the OSLC resources, their properties and constraints, as well as the OSLC services. For separation of concers, Lyo Designer provides three views for modeling different parts of the final adaptor:

\begin{itemize}
  \item Domain Specification View - for modeling the OSLC resources, their properties and relationships between them
  \item Toolchain View - for modeling the relationships between separate adaptors and resources, they consume and produce
  \item Adapter Interface View - for modeling the services and capabilities of a single adaptor
\end{itemize}

Lyo Designer also contains an utility called Code Generator, which is capable of generating code skeletons, compliant with OSLC, from the graphical models of the adaptors designed in Lyo Designer. The generated code is based on the OSLC4J SDK and can be used as a starting point for the implementation of the adaptor. The generated code contains designated places, where the developer should add code, which provides the functionality for the adaptor. This allows the code to be regenarated, upon the changes to the model, without breaking or losing any of the underlying implementation and custom code.

%=========================================================================

\chapter{Adaptor Design}
%\todo{Gramar check}
%\todo{Style document}

%\todo{Návrh v lyu}
%\todo{Rozdělení na Jira a R4J adaptory}
%\todo{Identifikátory Requirement a RequirementCollection}
%\todo{Mapping of properties}

%=========================================================================

\chapter{Implementation}
%\todo{Gramar check}
%\todo{Style document}

%\todo{Authentizace Basic + OAuth, not sure if here nebo v Adaptor Design zmínit jak to generuje Lyo a jak to používáme/bylo změno}
%\todo{Popis toho jaké jdou dělat query}
%\todo{Konfigurační soubory pro Jiru a R4J}
%\todo{Možnost uložení identifikátorů v labels fieldu}

%=========================================================================

\chapter{Evaluation and Testing}
%\todo{Gramar check}
%\todo{Style document}

%\todo{Testování v Postmanovi}
%\todo{Testování pomocí ReqIF souboru}
%\todo{Limitace + co dělat když nejde něco přidat do Jiry, permision needed for user to do CRUD operations}
%\todo{Pokud požiju R4J hledám pomocí folder a získávám jenom Bugs/Reqs které jsou  nalinkované v R4J, pokud využitu JIRA adaptor získávám všechny issues daného typu (v configuraci) -> JIRA adaptor potřebuje speciální issues types aby rozlišoval mezi req a jinými issues}
%\todo{req a reqCollection type name nesmí být stejné}
%\todo{BUGv R4J API: 500 returned při updatu folderu s parent=ROOT}
%možnost přidat OIDC nebo OAuth PKCE do budoucna

%=========================================================================

\chapter{Conclusion}
%\todo{Gramar check}
%\todo{Style document}

%=========================================================================

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it
% \end{document}